{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "609072ac-a034-4495-8bfc-616bb91d7d46",
   "metadata": {},
   "source": [
    "# Evaluating the use of Semi-Supervised Learning\n",
    "## for hate speech and offensive language\n",
    "*By Gabriel Pi≈°vejc*\n",
    "\n",
    "\n",
    "Using exTwitter data, I try to assess the possibility of using non labeled data to improve a transformer-encoder based model for classification.\n",
    "\n",
    "In particular, we will try to predict whether posts are considered hate speech, offensive language or neither. This has a level of difficulty, as the the two categories of interest of interest often overlap. The distinction is however very important as offensive language is but a cultural perception of certain words as less prestigious, while hate speech can go as far as to be a criminal offense in certain jurisdictions and is usually used in order to discriminate. There is an important difference between saying *let's fucking do this* and *the fucking [ethnicity of your choice] did this*.\n",
    "\n",
    "--------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35409a3b-13d8-49d5-84d3-e1a9e7bd2af2",
   "metadata": {},
   "source": [
    "First things first, we will need to load the [data](https://www.kaggle.com/datasets/mrmorj/hate-speech-and-offensive-language-dataset). While the original dataset comes with some additional information about the labeling process and the text appears in raw form, I have already preprocessed it outside of this notebook.\n",
    "\n",
    "In particular, I only maintained the labels and the text, as these are the two variables of interest for this particular report. Aditionally, I already passed the raw text through the transformer blocks of the [all-MiniLM-L6-v2 model](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2) which was designed to return the sentence embeddings. While it would be interesting to fine-tune this model for the task at hand, I decided against it as I am working on a small cuda-less notebook. Therefore, I only used the model to get the embeddings for each datapoint and we will do the SSL using a custom head for the model.\n",
    "\n",
    "For more information on the data preprocessing, see the `process_data.py` script.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6f6e210-f4f7-4600-aeec-efd0248c61aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"data/processed_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743426d7-b699-42f9-a9c7-bc70058e25d0",
   "metadata": {},
   "source": [
    "While we will of course do the traditional train/dev/test split, in this particular example this comes with a caveat. The three classes are not equally represented and it would be really easy to end up having a test or a dev set nearly (or even completely) lacking a certain label.\n",
    "\n",
    "A similar problem comes to the surface with the missing labels, as right now, the dataset is not missing any. It is not hard to mask them artifitially in order to run the experiments, however, at say ~90% masking rate it would be really easy to deepen the already deep representation problems.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06394dc6-9a7f-4cac-a588-a73e08dc3ec4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384\n",
       "1.0    0.774321\n",
       "2.0    0.167978\n",
       "0.0    0.057701\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# We can see the label proportions in the full dataset\n",
    "data.iloc[:, len(data.columns) - 1].value_counts(normalize=True)\n",
    "\n",
    "# 0 - Hate speech\n",
    "# 1 - Offensive language\n",
    "# 2 - Neither\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203aef1a-1bf3-4161-b28d-352ed0e812cf",
   "metadata": {},
   "source": [
    "In order to prevent these issues, I have created 2 custom functions to extract and mask certain proportions of the data label-wise. That is, if I decide to do 50% masking, it will mask 50% of each label, rather then doing it blindly. The same applies to creating splits. Therefore, there will be no need to worry about label representation in any split.\n",
    "\n",
    "Having said that, we will create the test split on 10% of the full dataset before masking. The train and dev sizes will be defined dynamically, as we will try the algorythm for different proportions of unlabeled data, however, the dev split will always be 10% of the train split size.\n",
    "\n",
    "For more information about the functions, see the `experiment_helpers.py`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10fe36b5-e269-4f90-88ef-99397b2880c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiment_helpers import mask_labels, extract_equal_proportion\n",
    "import torch\n",
    "\n",
    "data, test = extract_equal_proportion(data, proportion=0.1)\n",
    "\n",
    "test_X = torch.tensor(test.values[:, :-1], dtype=torch.float32)\n",
    "test_Y = torch.tensor(test.values[:, -1], dtype=torch.long)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3279243f-eae3-4785-a1cc-da9e0d975bd8",
   "metadata": {},
   "source": [
    "Aside from the data, I need a model to play with, or to be more precise, the head of the model, as we have already established that the transformer blocks and embeddings will stay intact.\n",
    "\n",
    "Having said that, I defined the head class in the `model_head.py` script in order to keep this notebook clean. We can use it to get a baseline with no masked data whatsoever, that is, the ideal scenario.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a6d9304-547f-48be-9b1d-09c9482266d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_head import model_head\n",
    "\n",
    "experiment_data = data.copy()\n",
    "\n",
    "train, dev = extract_equal_proportion(experiment_data, proportion=0.1)\n",
    "\n",
    "train_X = torch.tensor(train.values[:, :-1], dtype=torch.float32)\n",
    "train_Y = torch.tensor(train.values[:, -1], dtype=torch.long)\n",
    "\n",
    "dev_X = torch.tensor(dev.values[:, :-1], dtype=torch.float32)\n",
    "dev_Y = torch.tensor(dev.values[:, -1], dtype=torch.long)\n",
    "\n",
    "model = model_head()\n",
    "\n",
    "model.train(train_X, train_Y, dev_X, dev_Y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56510d01-e4bf-4dad-b199-3c4e7db6a77f",
   "metadata": {},
   "source": [
    "I have also written a custom evaluation function which returns the overall accuracy and per label recall. More information can once again be found in the the `experiment_helpers.py` script.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "563c5d07-6912-4d9e-a755-d6a7f5effacc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall accuracy: 0.887\n",
      "Recall on hate speech: 0.280\n",
      "Recall on offensive language: 0.954\n",
      "Recall on neither: 0.788\n"
     ]
    }
   ],
   "source": [
    "from experiment_helpers import evaluate_model\n",
    "\n",
    "predictions = model.predict(test_X, return_predictions=True)\n",
    "\n",
    "accuracy, recall_0, recall_1, recall_2 = evaluate_model(\n",
    "    model=model, predictions=predictions, data=test_X, ground_truth=test_Y\n",
    ")\n",
    "\n",
    "print(f\"Overall accuracy: {accuracy:.3f}\")\n",
    "print(f\"Recall on hate speech: {recall_0:.3f}\")\n",
    "print(f\"Recall on offensive language: {recall_1:.3f}\")\n",
    "print(f\"Recall on neither: {recall_2:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6808e556-7b1d-486b-b9fb-45e9b896ce1f",
   "metadata": {},
   "source": [
    "We can notice that even before dealing with missing labels, the model finds it hard to deal with the data imbalance. We can therefore \\[I guess oversample] in order to reduce this problem. Since I will be masking the labels by a constat percentage, the lacking representation will be a persistent problem across all scenarios. To mitigate this proble, will also repeat the \\[oversampling] step in every scenario.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45aa6b6-3fc7-4431-a84d-9905072bb877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I will be calling each proportion of masked data a scenario\n",
    "scenarios = [0.9, 0.75, 0.5, 0.25, 0.10]\n",
    "\n",
    "results = pd.DataFrame(\n",
    "    columns=[\n",
    "        \"scenario\",\n",
    "        \"accNoSSL\",\n",
    "        \"rec0NoSSL\",\n",
    "        \"rec1NoSSL\",\n",
    "        \"rec2NoSSL\",\n",
    "        \"accSSL\",\n",
    "        \"rec0SSL\",\n",
    "        \"rec1SSL\",\n",
    "        \"rec2SSL\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "index = 0\n",
    "for scenario in scenarios:\n",
    "\n",
    "    # Write down the current scenario\n",
    "    results.iloc[index, 0] = scenario\n",
    "\n",
    "    # Define the data situation for the scenario\n",
    "\n",
    "    experiment_data = data.copy()\n",
    "\n",
    "    experiment_data = mask_labels(experiment_data, mask_probability=scenario)\n",
    "    train, dev = extract_equal_proportion(experiment_data, proportion=0.1)\n",
    "\n",
    "    dev_X = torch.tensor(dev.values[:, :-1])\n",
    "    dev_Y = torch.tensor(dev.values[:, -1])\n",
    "\n",
    "    unlabeled_data, labeled_data = extract_equal_proportion(\n",
    "        experiment_data, proportion=1\n",
    "    )\n",
    "\n",
    "    # Train a model without SSL\n",
    "\n",
    "    train_X = torch.tensor(labeled_data.values[:, :-1], dtype=torch.float32)\n",
    "    train_Y = torch.tensor(labeled_data.values[:, -1], dtype=torch.long)\n",
    "\n",
    "    model = model_head()\n",
    "\n",
    "    model.train(train_X, train_Y, dev_X, dev_Y)\n",
    "\n",
    "    predictions = model.predict(test_X, return_predictions=True)\n",
    "\n",
    "    accuracy, recall_0, recall_1, recall_2 = evaluate_model(\n",
    "        model=model, predictions=predictions, data=test_X, ground_truth=test_Y\n",
    "    )\n",
    "\n",
    "    results.iloc[index, 1] = accuracy\n",
    "    results.iloc[index, 2] = recall_0\n",
    "    results.iloc[index, 3] = recall_1\n",
    "    results.iloc[index, 4] = recall_2\n",
    "\n",
    "    # <Train a model with SSL>\n",
    "\n",
    "    while True:\n",
    "        unlabeled_predictors = torch.tensor(dev.unlabeled_data[:, :-1])\n",
    "        labeled_predictors = torch.tensor(dev.labeled_data[:, :-1])\n",
    "        labels = torch.tensor(dev.labeled_data[:, -1])\n",
    "\n",
    "        # Train the model on labeled\n",
    "\n",
    "        # Predict on unlabeled\n",
    "\n",
    "        # if any <90% probs:\n",
    "\n",
    "        # take them into a new df\n",
    "\n",
    "        # Promote them\n",
    "\n",
    "        # Join this df into the labeled\n",
    "\n",
    "        # else:\n",
    "\n",
    "        # break\n",
    "\n",
    "    # Once we get here, this is the final model.\n",
    "    # Evaluate\n",
    "    # </Train a model with SSL>\n",
    "    # Write a line into the results table\n",
    "\n",
    "    index += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15edf387-40d2-4a56-b4d7-ae330a72a42f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
