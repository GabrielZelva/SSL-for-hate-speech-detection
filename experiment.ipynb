{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "609072ac-a034-4495-8bfc-616bb91d7d46",
   "metadata": {},
   "source": [
    "# Evaluating the use of Semi-Supervised Learning\n",
    "## for hate speech and offensive language\n",
    "*By Gabriel PiÅ¡vejc*\n",
    "\n",
    "\n",
    "Using exTwitter data, I try to assess the possibility of using non labeled data to improve a transformer-encoder based model for classification. \n",
    "\n",
    "In particular, we will try to predict whether posts are considered hate speech, offensive language or neither. This has a level of difficulty, as the the two categories of interest of interest often overlap. The distinction is however very important as offensive language is but a cultural perception of certain words as less prestigious, while hate speech can go as far as to be a criminal offense in certain jurisdictions and is usually used in order to discriminate. There is an important difference between saying *let's fucking do this* and *the fucking [ethnicity of your choice] did this*.  \n",
    "\n",
    "--------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35409a3b-13d8-49d5-84d3-e1a9e7bd2af2",
   "metadata": {},
   "source": [
    "First things first, we will need to load the [data](https://www.kaggle.com/datasets/mrmorj/hate-speech-and-offensive-language-dataset). While the original dataset comes with some additional information about the labeling process and the text appears in raw form, I have already preprocessed it outside of this notebook.\n",
    "\n",
    "In particular, I only maintained the labels and the text, as these are the two variables of interest for this particular report. Aditionally, I already passed the raw text through the transformer blocks of the [all-MiniLM-L6-v2 model](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2) which was designed to return the sentence embeddings. While it would be interesting to fine-tune this model for the task at hand, I decided against it as I am working on a small cuda-less notebook. Therefore, I only used the model to get the embeddings for each datapoint and we will do the SSL using a custom head for the model. \n",
    "\n",
    "For more information on the data preprocessing, see the `process_data.py` script.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6f6e210-f4f7-4600-aeec-efd0248c61aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('data/processed_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743426d7-b699-42f9-a9c7-bc70058e25d0",
   "metadata": {},
   "source": [
    "While we will of course do the traditional train/dev/test split, in this particular example this comes with a caveat. The three classes are not equally represented and it would be really easy to end up having a test or a dev set nearly (or even completely) lacking a certain label. \n",
    "\n",
    "A similar problem comes to the surface with the missing labels, as right now, the dataset is not missing any. It is not hard to mask them artifitially in order to run the experiments, however, at say ~90% masking rate it would be really easy to deepen the already deep representation problems.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06394dc6-9a7f-4cac-a588-a73e08dc3ec4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384\n",
       "1.0    0.774321\n",
       "2.0    0.167978\n",
       "0.0    0.057701\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# We can see the label proportions in the full dataset\n",
    "data.iloc[:, len(data.columns) - 1].value_counts(normalize=True)\n",
    "\n",
    "# 0 - Hate speech\n",
    "# 1 - Offensive language\n",
    "# 2 - Neither\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203aef1a-1bf3-4161-b28d-352ed0e812cf",
   "metadata": {},
   "source": [
    "In order to prevent these issues, I have created 2 custom functions to extract and mask certain proportions of the data label-wise. That is, if I decide to do 50% masking, it will mask 50% of each label, rather then doing it blindly. The same applies to creating splits. Therefore, there will be no need to worry about label representation in any split. \n",
    "\n",
    "Having said that, we will create the test split on 10% of the full dataset before masking. The train and dev sizes will be defined dynamically, as we will try the algorythm for different proportions of unlabeled data, however, the dev split will always be 10% of the train split size.\n",
    "\n",
    "For more information about the functions, see the `experiment_helpers.py`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10fe36b5-e269-4f90-88ef-99397b2880c3",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'columns'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m test_X = torch.tensor(test.values[:, :-\u001b[32m1\u001b[39m])\n\u001b[32m      7\u001b[39m test_Y = torch.tensor(test.values[:, -\u001b[32m1\u001b[39m])\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[43mtest_X\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m))\n",
      "\u001b[31mAttributeError\u001b[39m: 'Tensor' object has no attribute 'columns'"
     ]
    }
   ],
   "source": [
    "from experiment_helpers import mask_labels, extract_equal_proportion\n",
    "import torch\n",
    "\n",
    "data, test = extract_equal_proportion(data, proportion = 0.1)\n",
    "\n",
    "test_X = torch.tensor(test.values[:, :-1])\n",
    "test_Y = torch.tensor(test.values[:, -1])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e99c24e6-751f-47e0-9bd0-ff80c04f3d01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn \n",
    "# Here be the experiment code\n",
    "\n",
    "# I will be calling each proportion of masked data a scenario\n",
    "scenarios = [0.9, 0.75, 0.5, 0.25, 0.10]\n",
    "\n",
    "for scenario in scenarios: \n",
    "\n",
    "    # Define the data situation for the scenario\n",
    "\n",
    "    experiment_data = data.copy()\n",
    "\n",
    "    experiment_data = mask_labels(experiment_data, mask_probability = scenario)\n",
    "    train, dev = extract_equal_proportion(experiment_data, proportion = 0.1)\n",
    "\n",
    "    train_X = torch.tensor(train.values[:, :-1])\n",
    "    train_Y = torch.tensor(train.values[:, -1])\n",
    "\n",
    "    dev_X = torch.tensor(dev.values[:, :-1])\n",
    "    dev_Y = torch.tensor(dev.values[:, -1])\n",
    "    \n",
    "    # Define a model\n",
    "    model = nn.Sequential(\n",
    "        nn.Linear(384, 600), nn.ReLU(), \n",
    "        nn.Linear(600, 3), nn.Softmax() \n",
    "    )\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
    "    loss_fn = nn.CELoss()\n",
    "\n",
    "    epochs = 100 \n",
    "    batch_size = 10 \n",
    "    batches_per_epoch = len(train_X) // batch_size\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for i in range(batches_per_epoch):\n",
    "            \n",
    "            # get a batch \n",
    "            start = i * batch_size\n",
    "            X_batch = train_X[start:start+batch_size]\n",
    "            Y_batch = train_Y[start:start+batch_size]\n",
    "            \n",
    "            # forward pass \n",
    "            Y_pred = model(X_batch)\n",
    "            loss = loss_fn(Y_pred, Y_batch)\n",
    "            \n",
    "            # backward pass \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    # Handle the output\n",
    "\n",
    "    # <Implement the promotion process>\n",
    "\n",
    "    # This is what needs to happen:\n",
    "\n",
    "    # Create dfs with the labeled data\n",
    "    # Create dfs with unlabeled data\n",
    "\n",
    "    # Train the model on labeled\n",
    "\n",
    "    # Predict on unlabeled\n",
    "\n",
    "    # Take <90% probs into a new df\n",
    "\n",
    "    # Promote them\n",
    "\n",
    "    # Join this df into the labeled\n",
    "\n",
    "    # Repeat until no promotion\n",
    "\n",
    "    # This is the last model\n",
    "    \n",
    "    #<\\Implement the promotion process>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6d9304-547f-48be-9b1d-09c9482266d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
