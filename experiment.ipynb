{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "609072ac-a034-4495-8bfc-616bb91d7d46",
   "metadata": {},
   "source": [
    "# Evaluating the use of Semi-Supervised Learning\n",
    "## for hate speech and offensive language\n",
    "*By Gabriel Pi≈°vejc*\n",
    "\n",
    "\n",
    "Using exTwitter data, I try to assess the possibility of using non labeled data to improve a transformer-encoder based model for classification.\n",
    "\n",
    "In particular, we will try to predict whether posts are considered hate speech, offensive language or neither. This has a level of difficulty, as the the two categories of interest of interest often overlap. The distinction is however very important as offensive language is but a cultural perception of certain words as less prestigious, while hate speech can go as far as to be a criminal offense in certain jurisdictions and is usually used in order to discriminate. There is an important difference between saying *let's fucking do this* and *the fucking [ethnicity of your choice] did this*.\n",
    "\n",
    "--------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35409a3b-13d8-49d5-84d3-e1a9e7bd2af2",
   "metadata": {},
   "source": [
    "First things first, we will need to load the [data](https://www.kaggle.com/datasets/mrmorj/hate-speech-and-offensive-language-dataset). While the original dataset comes with some additional information about the labeling process and the text appears in raw form, I have already preprocessed it outside of this notebook.\n",
    "\n",
    "In particular, I only maintained the labels and the text, as these are the two variables of interest for this particular report. Aditionally, I already passed the raw text through the transformer blocks of the [all-MiniLM-L6-v2 model](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2) which was designed to return the sentence embeddings. While it would be interesting to fine-tune this model for the task at hand, I decided against it as I am working on a small cuda-less notebook. Therefore, I only used the model to get the embeddings for each datapoint and we will do the SSL using a custom head for the model.\n",
    "\n",
    "For more information on the data preprocessing, see the `process_data.py` script.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e6f6e210-f4f7-4600-aeec-efd0248c61aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"data/processed_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743426d7-b699-42f9-a9c7-bc70058e25d0",
   "metadata": {},
   "source": [
    "While we will of course do the traditional train/dev/test split, in this particular example this comes with a caveat. The three classes are not equally represented and it would be really easy to end up having a test or a dev set nearly (or even completely) lacking a certain label.\n",
    "\n",
    "A similar problem comes to the surface with the missing labels, as right now, the dataset is not missing any. It is not hard to mask them artifitially in order to run the experiments, however, at say ~90% masking rate it would be really easy to deepen the already deep representation problems.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "06394dc6-9a7f-4cac-a588-a73e08dc3ec4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384\n",
       "1.0    0.774321\n",
       "2.0    0.167978\n",
       "0.0    0.057701\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# We can see the label proportions in the full dataset\n",
    "data.iloc[:, len(data.columns) - 1].value_counts(normalize=True)\n",
    "\n",
    "# 0 - Hate speech\n",
    "# 1 - Offensive language\n",
    "# 2 - Neither\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203aef1a-1bf3-4161-b28d-352ed0e812cf",
   "metadata": {},
   "source": [
    "In order to prevent these issues, I have created 2 custom functions to extract and mask certain proportions of the data label-wise. That is, if I decide to do 50% masking, it will mask 50% of each label, rather then doing it blindly. The same applies to creating splits. Therefore, there will be no need to worry about label representation in any split.\n",
    "\n",
    "Having said that, we will create the test split on 10% of the full dataset before masking. The train and dev sizes will be defined dynamically, as we will try the algorythm for different proportions of unlabeled data, however, the dev split will always be 10% of the train split size.\n",
    "\n",
    "For more information about the functions, see the `experiment_helpers.py`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "10fe36b5-e269-4f90-88ef-99397b2880c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiment_helpers import mask_labels, extract_equal_proportion\n",
    "import torch\n",
    "\n",
    "data, test = extract_equal_proportion(data, proportion=0.1)\n",
    "\n",
    "test_X = torch.tensor(test.values[:, :-1], dtype=torch.float32)\n",
    "test_Y = torch.tensor(test.values[:, -1], dtype=torch.long)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e99c24e6-751f-47e0-9bd0-ff80c04f3d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class model_head:\n",
    "    def __init__(self):\n",
    "        self.model = nn.Sequential(nn.Linear(384, 600), nn.ReLU(), nn.Linear(600, 3))\n",
    "\n",
    "    def train(\n",
    "        self,\n",
    "        train_X,\n",
    "        train_Y,\n",
    "        dev_X,\n",
    "        dev_Y,\n",
    "        max_epochs=100,\n",
    "        batch_size=10,\n",
    "        patience=5,\n",
    "        learning_rate=1e-3,\n",
    "    ):\n",
    "        # Save the data\n",
    "        self.train_X = train_X\n",
    "        self.train_Y = train_Y\n",
    "        self.dev_X = dev_X\n",
    "        self.dev_Y = dev_Y\n",
    "\n",
    "        # Config\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "        batches_per_epoch = len(self.train_X) // batch_size\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        best_dev_loss = float(\"inf\")\n",
    "        epochs_without_improvement = 0\n",
    "\n",
    "        # Core loop\n",
    "        for epoch in range(max_epochs):\n",
    "            self.model.train()\n",
    "            for i in range(batches_per_epoch):\n",
    "                # get a batch\n",
    "                start = i * batch_size\n",
    "                X_batch = self.train_X[start : start + batch_size]\n",
    "                Y_batch = self.train_Y[start : start + batch_size]\n",
    "\n",
    "                # forward pass\n",
    "                Y_pred = self.model(X_batch)\n",
    "                loss = loss_fn(Y_pred, Y_batch)\n",
    "\n",
    "                # backward pass\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            self.model.eval()\n",
    "\n",
    "            # Early stopping check\n",
    "            with torch.no_grad():\n",
    "                dev_pred = self.model(self.dev_X)\n",
    "                dev_loss = loss_fn(dev_pred, self.dev_Y).item()\n",
    "\n",
    "            if dev_loss < best_dev_loss:\n",
    "                best_dev_loss = dev_loss\n",
    "                epochs_without_improvement = 0\n",
    "            else:\n",
    "                epochs_without_improvement += 1\n",
    "\n",
    "            if epochs_without_improvement >= patience:\n",
    "                break\n",
    "\n",
    "    def predict(\n",
    "        self, test_predictors, return_probabilities=False, return_predictions=False\n",
    "    ):\n",
    "        self.model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = self.model(test_predictors)\n",
    "            probabilities = torch.softmax(logits, dim=1)\n",
    "            predictions = torch.argmax(logits, dim=1)\n",
    "\n",
    "        if return_probabilities and return_predictions:\n",
    "            return probabilities, predictions\n",
    "\n",
    "        if return_probabilities:\n",
    "            return probabilities\n",
    "\n",
    "        if return_predictions:\n",
    "            return predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8a6d9304-547f-48be-9b1d-09c9482266d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[7.3257e-05, 5.0241e-02, 9.4969e-01]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing if the model works\n",
    "\n",
    "experiment_data = data.copy()\n",
    "\n",
    "train, dev = extract_equal_proportion(experiment_data, proportion=0.1)\n",
    "\n",
    "train_X = torch.tensor(train.values[:, :-1], dtype=torch.float32)\n",
    "train_Y = torch.tensor(train.values[:, -1], dtype=torch.long)\n",
    "\n",
    "dev_X = torch.tensor(dev.values[:, :-1], dtype=torch.float32)\n",
    "dev_Y = torch.tensor(dev.values[:, -1], dtype=torch.long)\n",
    "\n",
    "model = model_head()\n",
    "\n",
    "model.train(train_X, train_Y, dev_X, dev_Y)\n",
    "\n",
    "model.predict(test_X[0].unsqueeze(0), return_probabilities=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409c3edc-cc5a-4921-a708-85700be22b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I will be calling each proportion of masked data a scenario\n",
    "scenarios = [0.9, 0.75, 0.5, 0.25, 0.10]\n",
    "\n",
    "# Create an empty dataframe with the following data\n",
    "# - scenario\n",
    "# - accuracy without SLL\n",
    "# - recall for each label without SSL\n",
    "# - accuracy with SLL\n",
    "# - recall for each label with SSL\n",
    "\n",
    "for scenario in scenarios:\n",
    "    # Define the data situation for the scenario\n",
    "\n",
    "    experiment_data = data.copy()\n",
    "\n",
    "    experiment_data = mask_labels(experiment_data, mask_probability=scenario)\n",
    "    train, dev = extract_equal_proportion(experiment_data, proportion=0.1)\n",
    "\n",
    "    dev_X = torch.tensor(dev.values[:, :-1])\n",
    "    dev_Y = torch.tensor(dev.values[:, -1])\n",
    "\n",
    "    unlabeled_data, labeled_data = extract_equal_proportion(\n",
    "        experiment_data, proportion=1\n",
    "    )\n",
    "\n",
    "    # <Train a model without SSL>\n",
    "\n",
    "    # Save the accuracy and recall per label\n",
    "\n",
    "    # </Train a model without SSL>\n",
    "\n",
    "    # <Train a model with SSL>\n",
    "\n",
    "    while True:\n",
    "        unlabeled_predictors = torch.tensor(dev.unlabeled_data[:, :-1])\n",
    "        labeled_predictors = torch.tensor(dev.labeled_data[:, :-1])\n",
    "        labels = torch.tensor(dev.labeled_data[:, -1])\n",
    "\n",
    "        # Train the model on labeled\n",
    "\n",
    "        # Predict on unlabeled\n",
    "\n",
    "        # if any <90% probs:\n",
    "\n",
    "        # take them into a new df\n",
    "\n",
    "        # Promote them\n",
    "\n",
    "        # Join this df into the labeled\n",
    "\n",
    "        # else:\n",
    "\n",
    "        # break\n",
    "\n",
    "    # Once we get here, this is the final model.\n",
    "    # Evaluate\n",
    "    # </Train a model with SSL>\n",
    "    # Write a line into the results table\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8f2a58-509e-4b68-82b8-fc7033b5ac07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
